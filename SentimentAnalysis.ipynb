{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LpAQeAK5eGCF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import csv\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from IPython import embed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Sentiment Analysis Dataset.csv')"
      ],
      "metadata": {
        "id": "RtYD6yN5lsJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)"
      ],
      "metadata": {
        "id": "hHXOrJxBeOO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "5rfuDvcNePpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().any()"
      ],
      "metadata": {
        "id": "p7ZYfAeYeRBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking out the negative comments from the train set\n",
        "data[data['Sentiment'] == 0].head(10)"
      ],
      "metadata": {
        "id": "FUtRi4R_eTLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking out the postive comments from the train set\n",
        "data[data['Sentiment'] == 1].head(10)"
      ],
      "metadata": {
        "id": "DNq8JKpKeV10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Sentiment'].value_counts().plot.bar(color = 'pink', figsize = (6, 4))"
      ],
      "metadata": {
        "id": "snmgbmDXeWec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the distribution of tweets in the data\n",
        "length_train = data['SentimentText'].str.len().plot.hist(color = 'pink', figsize = (6, 4))"
      ],
      "metadata": {
        "id": "kspekFhTeYfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('Sentiment').describe()"
      ],
      "metadata": {
        "id": "crekZ4XDeaYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_str(string):\n",
        "    # EMOJIS\n",
        "    string = re.sub(r\":\\)\", \"emojihappy1\", string)\n",
        "    string = re.sub(r\":P\", \"emojihappy2\", string)\n",
        "    string = re.sub(r\":p\", \"emojihappy3\", string)\n",
        "    string = re.sub(r\":>\", \"emojihappy4\", string)\n",
        "    string = re.sub(r\":3\", \"emojihappy5\", string)\n",
        "    string = re.sub(r\":D\", \"emojihappy6\", string)\n",
        "    string = re.sub(r\" XD \", \"emojihappy7\", string)\n",
        "    string = re.sub(r\" <3 \", \"emojihappy8\", string)\n",
        "    string = re.sub(r\":\\(\", \"emojisad9\", string)\n",
        "    string = re.sub(r\":<\", \"emojisad10\", string)\n",
        "    string = re.sub(r\":<\", \"emojisad11\", string)\n",
        "    string = re.sub(r\">:\\(\", \"emojisad12\", string)\n",
        "\n",
        "    # MENTIONS\n",
        "    string = re.sub(r\"(@)\\w+\", \"\", string)\n",
        "\n",
        "    # WEBSITES\n",
        "    string = re.sub(r\"http(s)*:(\\S)*\", \"linktoken\", string)\n",
        "\n",
        "    # STRANGE UNICODE\n",
        "    string = re.sub(r\"\\\\x(\\S)*\", \"\", string)\n",
        "\n",
        "    # General Cleanup and Symbols\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    return string.strip().lower()\n"
      ],
      "metadata": {
        "id": "lhTbNHKJeb4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text'] = data['SentimentText'].apply(clean_str)"
      ],
      "metadata": {
        "id": "nEpFvRbqegkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "EhfP-auKeime"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(stop_words = 'english')\n",
        "words = cv.fit_transform(data.clean_text)\n",
        "sum_words = words.sum(axis=0)\n",
        "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
        "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'blue')\n",
        "plt.title(\"Most Frequently Occuring Words - Top 30\")"
      ],
      "metadata": {
        "id": "AfWKEO42ej3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "0YYNDGG2emC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(background_color = 'white', width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.title(\"WordCloud - Vocabulary from Dataset\", fontsize = 22)"
      ],
      "metadata": {
        "id": "7aJkTbgKen2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words =' '.join([text for text in data['clean_text'][data['Sentiment'] == 1]])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state = 0, max_font_size = 110).generate(positive_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Positive Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_QsHqC5yerY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words =' '.join([text for text in data['clean_text'][data['Sentiment'] == 0]])\n",
        "wordcloud = WordCloud(width=800, height=500, random_state = 0, max_font_size = 110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Negative Words')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iL3SCCuTetUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "fbZdofFwe1Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from textblob import TextBlob"
      ],
      "metadata": {
        "id": "rtkRtgeCe3-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def form_sentence(tweet):\n",
        "  tweet_blob = TextBlob(tweet)\n",
        "  return ' '.join(tweet_blob.words)\n",
        "print(form_sentence(data['SentimentText'].iloc[0]))\n",
        "print(data['SentimentText'].iloc[0])"
      ],
      "metadata": {
        "id": "7emFPm-ye5tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def no_user_alpha(tweet):\n",
        "  tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
        "  clean_tokens = [ele for ele in tweet.split() if re.match(r'[^\\W\\d]*$', ele)]\n",
        "  clean_s = ' '.join(clean_tokens)\n",
        "  clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
        "  return ' '.join(clean_mess)\n",
        "print(no_user_alpha(form_sentence(data['SentimentText'].iloc[0])))\n",
        "print(data['SentimentText'].iloc[0])"
      ],
      "metadata": {
        "id": "qMxEBduWe8Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalization(tweet_list):\n",
        "  lem = WordNetLemmatizer()\n",
        "  normalized_tweet = []\n",
        "  for word in tweet_list:\n",
        "    normalized_text = lem.lemmatize(word,'v')\n",
        "    normalized_tweet.append(normalized_text)\n",
        "  return ' '.join(normalized_tweet)\n",
        "print(normalization(data['SentimentText'].iloc[0].split()))\n",
        "print(data['SentimentText'].iloc[0])"
      ],
      "metadata": {
        "id": "oIOr8lhOfHOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text'] = data['clean_text'].apply(form_sentence)"
      ],
      "metadata": {
        "id": "AhtdU44VffwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text'] = data['clean_text'].apply(normalization)"
      ],
      "metadata": {
        "id": "DcskwTuOfhVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "my63XvOOfiqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words =' '.join([text for text in train['clean_text'][train['Sentiment'] == 1]])\n",
        "wordcloud = WordCloud(background_color = 'grey',width=800, height=500, random_state = 0, max_font_size = 110).generate(positive_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Positive Words after Cleaning')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DXS4LtPefkfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words =' '.join([text for text in data['clean_text'][data['Sentiment'] == 0]])\n",
        "wordcloud = WordCloud(background_color = 'grey',width=800, height=500, random_state = 0, max_font_size = 110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('The Negative Words after Cleaning ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mu3QrS0vfm6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separates a file with mixed positive and negative examples into two.\n",
        "def separate_dataset(filename):\n",
        "  good_out = open(\"good_file\",\"w+\",encoding=\"utf8\");\n",
        "  bad_out = open(\"bad_file\",\"w+\",encoding=\"utf8\");\n",
        "  seen = 1;\n",
        "  with open(filename,'r',encoding=\"utf8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    for line in reader:\n",
        "      seen +=1\n",
        "      sentiment = line[1]\n",
        "      sentence = line[4]\n",
        "      if (sentiment == \"0\"):\n",
        "        bad_out.write(sentence+\"\\n\")\n",
        "      else:\n",
        "        good_out.write(sentence+\"\\n\")\n",
        "      if (seen%10000==0):\n",
        "        print (seen);\n",
        "  good_out.close();\n",
        "  bad_out.close();"
      ],
      "metadata": {
        "id": "JBrd_Dc5foyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "separate_dataset(\"Sentiment Analysis Dataset.csv\");"
      ],
      "metadata": {
        "id": "NPte73Brft1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Datafiles\n",
        "def get_dataset(goodfile,badfile,limit,randomize=True):\n",
        "  good_x = list(open(goodfile,\"r\",encoding=\"utf8\").readlines())\n",
        "  good_x = [s.strip() for s in good_x]\n",
        "  bad_x = list(open(badfile,\"r\",encoding=\"utf8\").readlines())\n",
        "  bad_x = [s.strip() for s in bad_x]\n",
        "  if (randomize):\n",
        "    random.shuffle(bad_x)\n",
        "    random.shuffle(good_x)\n",
        "  good_x = good_x[:limit]\n",
        "  bad_x = bad_x[:limit]\n",
        "  x = good_x + bad_x\n",
        "  x = [clean_str(s) for s in x]\n",
        "  positive_labels = [[0, 1] for _ in good_x]\n",
        "  negative_labels = [[1, 0] for _ in bad_x]\n",
        "  y = np.concatenate([positive_labels, negative_labels], 0)\n",
        "  return [x,y]"
      ],
      "metadata": {
        "id": "391xfcvYfwEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate random batches\n",
        "def gen_batch(data, batch_size, num_epochs, shuffle=True): \"\"\"\n",
        "Generates a batch iterator for a dataset.\n",
        "\"\"\"\n",
        "  data = np.array(data)\n",
        "  data_size = len(data)\n",
        "  num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "  for epoch in range(num_epochs):\n",
        "  # Shuffle the data at each epoch\n",
        "    if shuffle:\n",
        "      shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "      shuffled_data = data[shuffle_indices]\n",
        "    else:\n",
        "      shuffled_data = data\n",
        "    for batch_num in range(num_batches_per_epoch):\n",
        "      start_index = batch_num * batch_size\n",
        "      end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "      yield shuffled_data[start_index:end_index]"
      ],
      "metadata": {
        "id": "txSr3o22fyID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation\n",
        "filename = \"Sentiment Analysis Dataset.csv\"\n",
        "goodfile = \"good_file\"\n",
        "badfile = \"bad_file\""
      ],
      "metadata": {
        "id": "6NRsoB8bf0Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_text, y = get_dataset(goodfile, badfile, 5000)"
      ],
      "metadata": {
        "id": "NghfFECDf10P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_tweets = pd.read_csv('good_file',error_bad_lines=False)"
      ],
      "metadata": {
        "id": "XRAHO1KAf3DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(good_tweets.shape)"
      ],
      "metadata": {
        "id": "w8uNZFaHf4eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_tweets.head(10)"
      ],
      "metadata": {
        "id": "wQr0_RxOf59P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_tweets = pd.read_csv('bad_file',error_bad_lines=False)"
      ],
      "metadata": {
        "id": "dPm184jBf7mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bad_tweets.shape)"
      ],
      "metadata": {
        "id": "BxKPGYPGf80x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_tweets.head(10)"
      ],
      "metadata": {
        "id": "KfZYQ0y9f99V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.14 import tensorflow as tf"
      ],
      "metadata": {
        "id": "Mc_0vrdHf_W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model Building"
      ],
      "metadata": {
        "id": "qB57GI7EgBhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from IPython import embed\n",
        "\n",
        "class CNN_LSTM(object):\n",
        "    def __init__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0, num_hidden=128):\n",
        "        # PLACEHOLDERS\n",
        "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\") # X - The Data\n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\") # Y - The Labels\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\") # Dropout\n",
        "\n",
        "        l2_loss = tf.constant(0.0) # Keeping track of L2 regularization loss\n",
        "\n",
        "        # 1. EMBEDDING LAYER ########################################\n",
        "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
        "            self.W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"W\")\n",
        "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
        "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
        "\n",
        "        # 2. CONVOLUTION LAYER + MAXPOOLING (per filter) ############\n",
        "        pooled_outputs = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
        "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
        "                conv = tf.nn.conv2d(self.embedded_chars_expanded, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
        "                # Non-linearity\n",
        "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
        "                # Maxpooling\n",
        "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
        "                pooled_outputs.append(pooled)\n",
        "\n",
        "        # Combining pooled features\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "\n",
        "        # 3. DROPOUT LAYER ###########################################\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
        "\n",
        "        # 4. LSTM LAYER ##############################################\n",
        "        cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
        "        self.h_drop_exp = tf.expand_dims(self.h_drop, 0)\n",
        "        val, state = tf.nn.dynamic_rnn(cell, self.h_drop_exp, dtype=tf.float32)\n",
        "\n",
        "        val2 = tf.transpose(val, [1, 0, 2])\n",
        "        last = tf.gather(val2, int(val2.get_shape()[0]) - 1)\n",
        "\n",
        "        out_weight = tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
        "        out_bias = tf.Variable(tf.random_normal([num_classes]))\n",
        "\n",
        "        with tf.name_scope(\"output\"):\n",
        "            self.scores = tf.nn.xw_plus_b(last, out_weight, out_bias, name=\"scores\")\n",
        "            self.predictions = tf.nn.softmax(self.scores, name=\"predictions\")\n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            self.losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
        "            self.loss = tf.reduce_mean(self.losses, name=\"loss\")\n",
        "\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            self.correct_pred = tf.equal(tf.argmax(self.predictions, 1), tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, \"float\"), name=\"accuracy\")\n",
        "\n",
        "        print(\"(!) LOADED CNN-LSTM! #\")\n"
      ],
      "metadata": {
        "id": "99UNUvh8gCXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "file_name = \"Sentiment Analysis Dataset.csv\"\n",
        "count = 1000\n",
        "\n",
        "subscript = 1\n",
        "\n",
        "while os.path.isfile('./good/' + str(count) + '_' + str(subscript)):\n",
        "    subscript += 1\n",
        "\n",
        "t_file = list(open(file_name, 'r', encoding=\"utf8\"))\n",
        "good_file = open(\"good\" + str(count) + '_' + str(subscript), 'a', encoding=\"utf8\")\n",
        "bad_file = open(\"bad\" + str(count) + '_' + str(subscript), 'a', encoding=\"utf8\")\n",
        "\n",
        "print(\"Opened files\")\n",
        "\n",
        "good_count = 0\n",
        "bad_count = 0\n",
        "\n",
        "while True:\n",
        "    line = random.choice(t_file)\n",
        "    line_split = line.split(',', 2)\n",
        "    label = int(line_split[1])\n",
        "    if label and good_count < count:\n",
        "        good_file.write(line)\n",
        "        good_count += 1\n",
        "    elif not label and bad_count < count:\n",
        "        bad_file.write(line)\n",
        "        bad_count += 1\n",
        "    elif bad_count >= count and good_count >= count:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "RtYLFIongi1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from tensorflow.contrib import learn\n",
        "from IPython import embed\n",
        "\n",
        "# Parameters\n",
        "dev_size = .10\n",
        "\n",
        "# Model Hyperparameters\n",
        "embedding_dim = 32  #128\n",
        "max_seq_length = 70\n",
        "filter_sizes = [3, 4, 5] #3\n",
        "num_filters = 32\n",
        "dropout_prob = 0.5 #0.5\n",
        "l2_reg_lambda = 0.0\n",
        "use_glove = True #00 we use glove\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "num_epochs = 10 #200\n",
        "evaluate_every = 100 #100\n",
        "checkpoint_every = 10000 #100\n",
        "num_checkpoints = 1 #Checkpoints to store\n",
        "\n",
        "# Misc Parameters\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "\n",
        "# Data Preparation\n",
        "filename = \"Sentiment Analysis Dataset.csv\"\n",
        "goodfile = \"good_file\"\n",
        "badfile = \"bad_file\"\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "x_text, y = get_dataset(goodfile, badfile, 5000)\n",
        "\n",
        "# TODO: MAX LENGTH\n",
        "# Build vocabulary\n",
        "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
        "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
        "\n",
        "# Randomly shuffle data\n",
        "np.random.seed(42)\n",
        "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "x_shuffled = x[shuffle_indices]\n",
        "y_shuffled = y[shuffle_indices]\n",
        "\n",
        "# Split train/test set\n",
        "# TODO: This is very crude, should use cross-validation\n",
        "dev_sample_index = -1 * int(dev_size * float(len(y)))\n",
        "x_train, x_test = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
        "y_train, y_test = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
        "\n",
        "print(\"Train/Test split: {:d}/{:d}\".format(len(y_train), len(y_test)))\n"
      ],
      "metadata": {
        "id": "BaJhynFsh0jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.ConfigProto(\n",
        "      allow_soft_placement=allow_soft_placement,\n",
        "      log_device_placement=log_device_placement)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        cnn = CNN_LSTM(\n",
        "            x_train.shape[1],\n",
        "            y_train.shape[1],\n",
        "            len(vocab_processor.vocabulary_),\n",
        "            embedding_dim,\n",
        "            filter_sizes,\n",
        "            num_filters,\n",
        "            l2_reg_lambda=l2_reg_lambda)\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        timestamp = str(int(time.time()))\n",
        "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
        "\n",
        "        # Write vocabulary\n",
        "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # TRAINING STEP\n",
        "        def train_step(x_batch, y_batch, save=False):\n",
        "            feed_dict = {\n",
        "              cnn.input_x: x_batch,\n",
        "              cnn.input_y: y_batch,\n",
        "              cnn.dropout_keep_prob: dropout_prob\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if save:\n",
        "                train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "        # EVALUATE MODEL\n",
        "        def test_step(x_batch, y_batch, writer=None, save=False):\n",
        "            feed_dict = {\n",
        "              cnn.input_x: x_batch,\n",
        "              cnn.input_y: y_batch,\n",
        "              cnn.dropout_keep_prob: 0.5\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if save:\n",
        "                if writer:\n",
        "                    writer.add_summary(summaries, step)\n",
        "\n",
        "        # CREATE THE BATCHES GENERATOR\n",
        "        batches = gen_batch(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
        "\n",
        "        # TRAIN FOR EACH BATCH\n",
        "        for batch in batches:\n",
        "            x_batch, y_batch = zip(*batch)\n",
        "            train_step(x_batch, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % evaluate_every == 0:\n",
        "                print(\"\\nEvaluation:\")\n",
        "                test_step(x_test, y_test, writer=dev_summary_writer)\n",
        "                print(\"\")\n",
        "            if current_step % checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
      ],
      "metadata": {
        "id": "i4x4flW1h6CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE THE BATCHES GENERATOR\n",
        "batches = gen_batch(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
        "\n",
        "# TRAIN FOR EACH BATCH\n",
        "for batch in batches:\n",
        "    x_batch, y_batch = zip(*batch)\n",
        "    train_step(x_batch, y_batch)\n",
        "    current_step = tf.train.global_step(sess, global_step)\n",
        "    if current_step % evaluate_every == 0:\n",
        "        print(\"\\nEvaluation:\")\n",
        "        test_step(x_test, y_test, writer=dev_summary_writer)\n",
        "        print(\"\")\n",
        "    if current_step % checkpoint_every == 0:\n",
        "        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "        print(\"Saved model checkpoint to {}\\n\".format(path))\n"
      ],
      "metadata": {
        "id": "bS7zHzAmiQUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}